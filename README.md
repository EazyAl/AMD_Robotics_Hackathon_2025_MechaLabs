**### Make a fork or copy of this repo and fill in your team submission details! ###**

# AMD_Robotics_Hackathon_2025_Mecha Labs

## Team Information

Team 7 
Mecha Labs
Ali Imran, Yiyang Lu, Victor Oldensand, Christopher Piggott 

**Summary:** *A brief description of your work*

MedBot is a companion + medical robot that can provide and administer medications such as an epipen. 

## Submission Details

### 1. Mission Description
- The goal was to build both a medical and companion bot for people with severe allergies, diabetes, and the elderly, by providing a LeKiwi robot with emotionally expressive capabilities and a medpack with epipens to support during allergic shocks as our first users. 

### 2. Creativity
- We modified the LeKiwi robot with medpacks (3d printed) and a custom adjusted overhead light and camera for better policy inference and training. We believe this creatively shows how both the mobile and manipulator components of the robot can fit into a real world use case. 

### 3. Technical implementations
For emotional expressiveness we created a multithreaded dispatch architecture of prerecorded emotes that were controlled by a livekit agent, behaviors and communicating with the users. 

For the medical application we implemented a sentry mode using mediapipe to identify pose landmarks and track the user. Then we used a depth map using a neural network for one camera to travel towards the leg of the owner (injection point). 

<img width="2048" height="945" alt="image" src="https://github.com/user-attachments/assets/94fb3a9c-bb04-4f5e-8c34-9625b39a9619" />


We also trained 13 different policies (VLAs and ACT) to run the administration process of the epipen, setting up camera angles and overhead light to improve its ability. We ultimately decided to use the ACT policy as it provided best performance to training time. 

<img width="2048" height="945" alt="image" src="https://github.com/user-attachments/assets/23af6ee9-ace5-42f5-9c58-2406ad1ba245" />

### 4. Ease of use
- Very easy to set up but as it is cabled now (didnt have time to run it with zmq messaging on the raspberry pi) it is limited in reach
- One command to run sentry mode using a uv python project

## Additional Links

**Hugging Face Latest Model**
https://huggingface.co/CRPlab/letars_test_policy_2

**Hugging Face Latest Dataset**
https://huggingface.co/datasets/CRPlab/letars-dataset-30

**VIDEOS**
https://drive.google.com/drive/folders/1DS3fAy32oV_LqFkvzqe3nGeA4KLl8XXg?usp=sharing


## Code submission

This is the directory tree of this repo, you need to fill in the `mission` directory with your submission details.

```terminal

```


The `latest-run` is generated by wandb for your training job. Please copy it into the wandb sub directory of you Hackathon Repo.

The whole dir of `latest-run` will look like below:

```terminal

```

**NOTES**

1. The `latest-run` is the soft link, please make sure to copy the real target directory it linked with all sub dirs and files.
2. Only provide (upload) the wandb of your last success pre-trained model for the Mission.
